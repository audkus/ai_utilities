# AI Utilities Environment Configuration
 # Copy this file to .env and uncomment what you need.
 
 # Default behavior:
 # - If AI_PROVIDER is unset or set to "auto", ai_utilities will select a configured provider.
 # - Provider-specific variables (e.g. OPENAI_API_KEY) take precedence over generic AI_*.
 
 # Optional global defaults
# AI_PROVIDER=auto
# AI_MODEL=gpt-4o-mini
# AI_LOG_LEVEL=INFO
# AI_AUTO_SELECT_ORDER=openai,groq,openrouter,together,ollama,fastchat,text-generation-webui
# Multiple providers can be configured simultaneously. AI_PROVIDER=auto will select
# the first usable provider based on available keys and AI_AUTO_SELECT_ORDER.
 
 # ===== HOSTED PROVIDERS =====
 
 # OpenAI
 # OPENAI_API_KEY=sk-...
 # OPENAI_BASE_URL=https://api.openai.com/v1
 # OPENAI_MODEL=gpt-4o-mini
 
 # Groq
 # GROQ_API_KEY=gsk_...
 # GROQ_BASE_URL=https://api.groq.com/openai/v1
 # GROQ_MODEL=llama3-70b-8192
 
 # Together
 # TOGETHER_API_KEY=tgp_...
 # TOGETHER_BASE_URL=https://api.together.xyz/v1
 # TOGETHER_MODEL=meta-llama/Llama-3-8b-chat-hf
 
 # OpenRouter
 # OPENROUTER_API_KEY=sk-or-v1-...
 # OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
 # OPENROUTER_MODEL=meta-llama/llama-3-8b-instruct:free
 
 # ===== LOCAL PROVIDERS (require a model) =====
 
 # Ollama
 # OLLAMA_BASE_URL=http://localhost:11434/v1
 # OLLAMA_MODEL=llama3.1
 
 # FastChat
 # FASTCHAT_BASE_URL=http://localhost:8000/v1
 # FASTCHAT_MODEL=your-model-name
 
 # Text Generation WebUI
# TEXT_GENERATION_WEBUI_BASE_URL=http://localhost:5000/v1
# TEXT_GENERATION_WEBUI_MODEL=your-model-name
# Depending on your deployment, the OpenAI-compatible endpoint may be exposed at /v1 
# or directly on the server port (e.g. 127.0.0.1:7860).
 
 # ===== LEGACY FALLBACKS (pre-1.0, backward compatibility only) =====
# Provider-specific variables are preferred for new setups
# AI_PROVIDER=openai
# AI_API_KEY=sk-...
# AI_BASE_URL=https://api.openai.com/v1
# AI_MODEL=gpt-4o-mini