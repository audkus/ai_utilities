# AI Utilities Environment Configuration
# Copy this file to .env and fill in your actual values

# ===== PRIMARY PROVIDER (for single-provider use) =====
# Choose ONE of these - this will be your default provider

# OpenAI (most reliable)
# AI_API_KEY=sk-your-openai-key-here

# Groq (fastest free tier - 8,000 requests/day)
# AI_API_KEY=gsk_your-groq-key-here

# Together AI ($5 free credits)
# AI_API_KEY=tgp_your-together-key-here

# OpenRouter ($5 free credits, access to all models)
# AI_API_KEY=sk-or-v1-your-openrouter-key-here

# Local LM Studio (requires LM Studio running)
# AI_BASE_URL=http://localhost:1234/v1
# AI_API_KEY=lm-studio

# Local Ollama (requires Ollama running)
# AI_BASE_URL=http://localhost:11434/v1
# AI_API_KEY=ollama

# ===== MULTI-PROVIDER CONFIGURATION =====
# Required for testing with multiple providers or specific provider access

# Individual provider keys (needed for testing, multi-provider scenarios)
GROQ_API_KEY=your-groq-key-here
TOGETHER_API_KEY=your-together-key-here
OPENROUTER_API_KEY=your-openrouter-key-here

# Local provider configurations
TEXT_GENERATION_WEBUI_API_KEY=your-webui-key-here  # optional
TEXT_GENERATION_WEBUI_BASE_URL=http://localhost:5000/v1
FASTCHAT_API_KEY=your-fastchat-key-here  # optional
FASTCHAT_BASE_URL=http://localhost:8000/v1

# ===== ADVANCED SETTINGS =====

# Override provider explicitly (usually not needed)
# AI_PROVIDER=openai

# Custom base URL for OpenAI-compatible endpoints
# AI_BASE_URL=https://api.openai.com/v1

# ===== USAGE EXAMPLES =====

# Basic usage with primary provider:
#   from ai_utilities import AiClient
#   client = AiClient()
#   response = client.ask("Hello")

# Specific provider configuration:
#   client = AiClient(
#       provider="groq",
#       model="llama3-70b-8192",
#       temperature=0.5
#   )

# Local provider setup:
#   client = AiClient(
#       provider="openai_compatible",
#       base_url="http://localhost:1234/v1",
#       api_key="lm-studio",
#       model="your-local-model"
#   )
