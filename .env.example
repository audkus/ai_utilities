# AI Utilities Environment Configuration
# Copy this file to .env and fill in your actual values
#
# IMPORTANT: Choose ONE provider configuration below and uncomment it
# The AI_MODEL and AI_TEMPERATURE should be set in your code, not here

# ===== CHOOSE YOUR PROVIDER =====

# Option 1: OpenAI (most reliable)
# Get your key from: https://platform.openai.com/api-keys
# AI_API_KEY=sk-your-openai-key-here

# Option 2: Groq (fastest free tier - 8,000 requests/day)
# Get your key from: https://console.groq.com/keys
# AI_API_KEY=gsk_your-groq-key-here

# Option 3: Together AI ($5 free credits)
# Get your key from: https://api.together.xyz/settings/api-keys
# AI_API_KEY=tgp_your-together-key-here

# Option 4: OpenRouter ($5 free credits, access to all models)
# Get your key from: https://openrouter.ai/keys
# AI_API_KEY=sk-or-v1-your-openrouter-key-here

# Option 5: Local LM Studio (requires LM Studio running)
# Make sure LM Studio is running with server enabled
# AI_BASE_URL=http://localhost:1234/v1
# AI_API_KEY=lm-studio

# Option 6: Local Ollama (requires Ollama running)
# Make sure Ollama is running: ollama serve
# AI_BASE_URL=http://localhost:11434/v1
# AI_API_KEY=ollama

# ===== LEGACY CONFIGURATIONS (for reference) =====
# These are kept for backward compatibility but not recommended for new setups

# Additional Cloud Provider API Keys (alternative method)
# GROQ_API_KEY=your-groq-key-here
# TOGETHER_API_KEY=your-together-key-here
# OPENROUTER_API_KEY=your-openrouter-key-here

# Local Server Configurations (alternative method)
# TEXT_GENERATION_WEBUI_API_KEY=your-webui-key-here (optional)
# TEXT_GENERATION_WEBUI_BASE_URL=http://localhost:5000/v1
# FASTCHAT_API_KEY=your-fastchat-key-here (optional)
# FASTCHAT_BASE_URL=http://localhost:8000/v1

# ===== ADVANCED SETTINGS =====

# Override provider explicitly (usually not needed)
# AI_PROVIDER=openai

# Custom base URL for OpenAI-compatible endpoints
# AI_BASE_URL=https://api.openai.com/v1

# ===== USAGE EXAMPLES =====
#
# Set model and temperature in your code:
#
#   from ai_utilities import AiClient
#   
#   # Use default settings
#   client = AiClient()
#   response = client.ask("Hello")
#   
#   # Or specify model and temperature
#   client = AiClient(model="gpt-4", temperature=0.5)
#   response = client.ask("Hello")
#
#   # For local providers, also set base_url
#   client = AiClient(
#       base_url="http://localhost:1234/v1",
#       api_key="lm-studio",
#       model="your-local-model"
#   )
