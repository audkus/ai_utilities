name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12"]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Lint with flake8
      run: |
        pip install flake8
        # Stop the build if there are Python syntax errors or undefined names
        flake8 src/ --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 src/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Type check with mypy
      run: |
        mypy src/ai_utilities/ --no-error-summary
    
    - name: Test with pytest
      run: |
        pytest tests/ -v --tb=short --cov=ai_utilities --cov-report=xml --cov-report=term-missing
    
    - name: Upload coverage to Codecov
      if: matrix.python-version == '3.9'
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  integration-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.9", "3.11", "3.12"]  # Test oldest, newest, and middle versions
    needs: test
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install package
      run: |
        pip install .
    
    - name: Test basic import
      run: |
        python -c "import ai_utilities; print('OK: Import works')"
    
    - name: Test examples
      run: |
        python examples/getting_started.py || echo "Expected failure without API key"
        python examples/model_validation.py || echo "Expected failure without API key"

  security-scan:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.9", "3.11", "3.12"]  # Test security across versions
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        pip install safety bandit
    
    - name: Check for security vulnerabilities
      run: |
        safety check
    
    - name: Run bandit security linter
      run: |
        bandit -r src/ -f json -o bandit-report.json || true
        bandit -r src/ --severity-level high  # Only fail on high severity issues

  build-docs:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -e ".[dev]"
    
    - name: Test documentation build
      run: |
        # Test that all examples can be imported
        python -c "
        import examples.getting_started
        import examples.model_validation
        print('OK: All examples import successfully')
        "

  performance-test:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install package
      run: |
        pip install .
    
    - name: Performance benchmarks
      run: |
        python -c "
        import time
        import os
        from ai_utilities import AiSettings, AiClient
        from ai_utilities.providers.base_provider import BaseProvider
        
        # Create a minimal mock provider for performance testing
        class MockProvider(BaseProvider):
            def ask(self, prompt, **kwargs):
                return 'Mock response'
            
            def ask_many(self, prompts, **kwargs):
                return ['Mock response'] * len(prompts)
            
            def upload_file(self, path, **kwargs):
                class MockFile:
                    def __init__(self):
                        self.file_id = 'mock-id'
                        self.filename = 'mock.txt'
                return MockFile()
            
            def download_file(self, file_id):
                return b'Mock content'
            
            def generate_image(self, prompt, **kwargs):
                return ['mock-url']
        
        # Set fake API key to avoid errors
        os.environ['AI_API_KEY'] = 'fake-key-for-performance-testing'
        
        # Test import speed
        start = time.time()
        from ai_utilities import AiClient, AiSettings, AskResult
        import_time = time.time() - start
        print(f'OK: Import time: {import_time:.3f}s')
        
        # Test object creation speed (with mock provider)
        start = time.time()
        settings = AiSettings(provider='openai')  # Provider name doesn't matter for mock
        client = AiClient(settings=settings, provider=MockProvider())
        creation_time = time.time() - start
        print(f'OK: Object creation time: {creation_time:.3f}s')
        
        # Performance assertions
        assert import_time < 1.0, f'Import too slow: {import_time}s'
        assert creation_time < 1.0, f'Object creation too slow: {creation_time}s'
        
        print('OK: Performance benchmarks passed')
        "

  compatibility-test:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.9", "3.11", "3.12"]  # Test cross-platform compatibility across versions
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install package
      run: |
        pip install .
    
    - name: Test cross-platform compatibility
      run: |
        python -c "
        import ai_utilities
        from ai_utilities import AiClient, AiSettings, AskResult
        print(f'OK: Platform: ${{ matrix.os }} works')
        "

  notify:
    runs-on: ubuntu-latest
    needs: [test, integration-test, security-scan, build-docs, performance-test, compatibility-test]
    if: always()
    
    steps:
    - name: Notify success
      if: ${{ needs.test.result == 'success' && needs.integration-test.result == 'success' && needs.security-scan.result == 'success' && needs.build-docs.result == 'success' && needs.performance-test.result == 'success' && needs.compatibility-test.result == 'success' }}
      run: |
        echo "SUCCESS: All CI checks passed!"
        echo "OK: Tests"
        echo "OK: Integration"
        echo "OK: Security"
        echo "OK: Documentation"
        echo "OK: Performance"
        echo "OK: Cross-platform compatibility"
    
    - name: Notify failure
      if: ${{ needs.test.result == 'failure' || needs.integration-test.result == 'failure' || needs.security-scan.result == 'failure' || needs.build-docs.result == 'failure' || needs.performance-test.result == 'failure' || needs.compatibility-test.result == 'failure' }}
      run: |
        echo "FAIL: CI checks failed!"
        echo "Test: ${{ needs.test.result }}"
        echo "Integration: ${{ needs.integration-test.result }}"
        echo "Security: ${{ needs.security-scan.result }}"
        echo "Docs: ${{ needs.build-docs.result }}"
        echo "Performance: ${{ needs.performance-test.result }}"
        echo "Compatibility: ${{ needs.compatibility-test.result }}"
        exit 1
